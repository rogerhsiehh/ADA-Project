{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0739456-d82e-4de9-a333-501ca09edde0",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04cfbada-6487-4319-ba0c-b05f7f506471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/p8ygjg1n0g35_jwbcczssrrr0000gn/T/ipykernel_34780/975642602.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_dataset = torch.load(\"train_dataset.pt\")\n",
      "/var/folders/kv/p8ygjg1n0g35_jwbcczssrrr0000gn/T/ipykernel_34780/975642602.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_dataset = torch.load(\"val_dataset.pt\")\n",
      "/var/folders/kv/p8ygjg1n0g35_jwbcczssrrr0000gn/T/ipykernel_34780/975642602.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_dataset = torch.load(\"test_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "from utils import CarDDDataset\n",
    "import torch\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = torch.load(\"train_dataset.pt\")\n",
    "val_dataset = torch.load(\"val_dataset.pt\")\n",
    "test_dataset = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "print(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e3b89a8-ce72-43a0-a1a5-5f2a9b0ae241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.Image.Image image mode=RGB size=1000x750>,\n",
       " 'image_file_path': 'CarDD_release/CarDD_COCO/train2017/000001.jpg',\n",
       " 'labels': tensor([0., 1., 0., 0., 0., 1.]),\n",
       " 'active_label_names': ['Scratch', 'Tire Flat']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635bb57b-c7f3-40d2-a563-dbb0a4afa724",
   "metadata": {},
   "source": [
    "# Loading ViT Feature Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454184dd-9813-4153-b7eb-af9e15fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "776abd5e-7f25-4613-800b-317c3c6a36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define feature extractor\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_processor = ViTImageProcessor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88c5903-5dbb-4a25-846c-2775c00b6924",
   "metadata": {},
   "source": [
    "# Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0f08acf-6e26-4cce-99d7-a59fb99156fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, feature_processor):\n",
    "        self.dataset = dataset\n",
    "        self.feature_processor = feature_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        # Apply feature processor to the image\n",
    "        processed = self.feature_processor(images=sample['image'], return_tensors='pt') #return_tensors='pt' argument, we'll get back torch tensors instead of numpy array\n",
    "        processed = {key: value.squeeze(0) for key, value in processed.items()}  # Remove batch dimension\n",
    "        processed['labels'] = sample['labels']  # Add labels\n",
    "        return processed\n",
    "\n",
    "# Step 3: Initialize CustomDataset\n",
    "processed_train_dataset = CustomDataset(train_dataset, feature_processor)\n",
    "processed_eval_dataset = CustomDataset(val_dataset, feature_processor)\n",
    "processed_test_dataset = CustomDataset(test_dataset, feature_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b814e4b7-f765-4169-abef-80691c8606e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 0.9451,  0.9608,  0.9686,  ...,  0.9922,  1.0000,  0.9922],\n",
       "          [ 0.9294,  0.9451,  0.9686,  ...,  0.9922,  1.0000,  1.0000],\n",
       "          [ 0.9451,  0.9529,  0.9608,  ...,  1.0000,  0.9922,  0.9922],\n",
       "          ...,\n",
       "          [ 0.2784,  0.2314,  0.3098,  ..., -0.4824, -0.5059, -0.5294],\n",
       "          [ 0.0039,  0.1373,  0.3176,  ..., -0.5294, -0.3882, -0.4667],\n",
       "          [ 0.2000,  0.1608,  0.2314,  ..., -0.5294, -0.4118, -0.4275]],\n",
       " \n",
       "         [[-0.6941, -0.6235, -0.6000,  ..., -0.2392, -0.2549, -0.2627],\n",
       "          [-0.6863, -0.6314, -0.5765,  ..., -0.2314, -0.2471, -0.2627],\n",
       "          [-0.6784, -0.6392, -0.6000,  ..., -0.2549, -0.2392, -0.2549],\n",
       "          ...,\n",
       "          [ 0.2863,  0.2314,  0.3255,  ..., -0.5529, -0.5843, -0.6000],\n",
       "          [ 0.0431,  0.1686,  0.3333,  ..., -0.5843, -0.4980, -0.5529],\n",
       "          [ 0.2471,  0.2000,  0.2392,  ..., -0.5765, -0.4824, -0.4980]],\n",
       " \n",
       "         [[-0.5451, -0.4980, -0.4510,  ..., -0.2941, -0.2863, -0.2863],\n",
       "          [-0.5529, -0.5137, -0.4588,  ..., -0.2784, -0.2863, -0.3098],\n",
       "          [-0.5373, -0.5137, -0.4824,  ..., -0.3020, -0.2706, -0.2941],\n",
       "          ...,\n",
       "          [ 0.3412,  0.3020,  0.4039,  ..., -0.6078, -0.6627, -0.6784],\n",
       "          [ 0.1294,  0.2549,  0.3961,  ..., -0.6392, -0.6392, -0.6784],\n",
       "          [ 0.3255,  0.2863,  0.3020,  ..., -0.6314, -0.5765, -0.5922]]]),\n",
       " 'labels': tensor([0., 1., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79cb988-8aa6-41d2-afe5-4b89aa132854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Collate Function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df14e83d-4916-462a-86a4-e6094c0ed734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Metric\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # Apply sigmoid to predictions to get probabilities\n",
    "    preds = torch.sigmoid(torch.tensor(p.predictions)).numpy()\n",
    "    # Apply a threshold to convert probabilities to binary predictions\n",
    "    threshold = 0.5\n",
    "    binary_preds = (preds > threshold).astype(int)\n",
    "    # Convert references to numpy\n",
    "    references = p.label_ids\n",
    "    # Compute accuracy for multi-label classification\n",
    "    # True if all labels match for a sample, False otherwise\n",
    "    sample_accuracies = (binary_preds == references).all(axis=1)\n",
    "    accuracy = sample_accuracies.mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4142d952-4e6a-4e41-b36b-5482ed10182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Load the ViT Model\n",
    "num_labels = train_dataset[0]['labels'].shape[0]\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label={str(i): f\"label_{i}\" for i in range(num_labels)},\n",
    "    label2id={f\"label_{i}\": i for i in range(num_labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75467948-e6f1-4478-a928-933f9e5b4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: ['classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "# Freeze the backbone\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Verify trainable parameters\n",
    "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "print(\"Trainable Parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce955524-ca42-44f6-b9b3-f05e565aad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-final-layer-only\",\n",
    "    per_device_train_batch_size=32,\n",
    "    eval_strategy=\"steps\",  # Updated\n",
    "    num_train_epochs=10,\n",
    "    bf16=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a8c7ddc-e5cc-40ab-afa9-3ce0967f5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_train_dataset,  # Training dataset\n",
    "    eval_dataset=processed_eval_dataset,  # Validation dataset\n",
    "    processing_class=feature_processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9512132e-4bd6-4be8-8b56-8a3ac201497a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='880' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [880/880 1:07:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>0.524162</td>\n",
       "      <td>0.250617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.464617</td>\n",
       "      <td>0.323457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.435600</td>\n",
       "      <td>0.435181</td>\n",
       "      <td>0.346914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.417407</td>\n",
       "      <td>0.362963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.419600</td>\n",
       "      <td>0.405659</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.397914</td>\n",
       "      <td>0.375309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.392902</td>\n",
       "      <td>0.375309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.390205</td>\n",
       "      <td>0.381481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27e1f573-8caf-4f2e-b29f-d8fe477a3a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         10.0\n",
      "  total_flos               = 2032381165GF\n",
      "  train_loss               =       0.4371\n",
      "  train_runtime            =   1:07:51.46\n",
      "  train_samples_per_second =        6.916\n",
      "  train_steps_per_second   =        0.216\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80ef0ada-43ee-403c-a5ee-b026ccd58c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.39020490646362305, 'eval_accuracy': 0.3814814814814815, 'eval_runtime': 93.5819, 'eval_samples_per_second': 8.656, 'eval_steps_per_second': 1.09, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation dataset\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d49fe07f-3e23-457a-bb04-cc1900e6ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard is running at http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import program\n",
    "\n",
    "logdir = './vit-final-layer-only' \n",
    "tb = program.TensorBoard()\n",
    "tb.configure(argv=['serve', '--logdir', logdir])\n",
    "url = tb.launch()\n",
    "print(f\"TensorBoard is running at {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e777f-2465-4517-b8ee-9aff240c1718",
   "metadata": {},
   "source": [
    "# Model Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68c3f6b5-9786-44cb-800a-64b7736d8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: ['vit.embeddings.cls_token', 'vit.embeddings.position_embeddings', 'vit.embeddings.patch_embeddings.projection.weight', 'vit.embeddings.patch_embeddings.projection.bias', 'vit.encoder.layer.0.attention.attention.query.weight', 'vit.encoder.layer.0.attention.attention.query.bias', 'vit.encoder.layer.0.attention.attention.key.weight', 'vit.encoder.layer.0.attention.attention.key.bias', 'vit.encoder.layer.0.attention.attention.value.weight', 'vit.encoder.layer.0.attention.attention.value.bias', 'vit.encoder.layer.0.attention.output.dense.weight', 'vit.encoder.layer.0.attention.output.dense.bias', 'vit.encoder.layer.0.intermediate.dense.weight', 'vit.encoder.layer.0.intermediate.dense.bias', 'vit.encoder.layer.0.output.dense.weight', 'vit.encoder.layer.0.output.dense.bias', 'vit.encoder.layer.0.layernorm_before.weight', 'vit.encoder.layer.0.layernorm_before.bias', 'vit.encoder.layer.0.layernorm_after.weight', 'vit.encoder.layer.0.layernorm_after.bias', 'vit.encoder.layer.1.attention.attention.query.weight', 'vit.encoder.layer.1.attention.attention.query.bias', 'vit.encoder.layer.1.attention.attention.key.weight', 'vit.encoder.layer.1.attention.attention.key.bias', 'vit.encoder.layer.1.attention.attention.value.weight', 'vit.encoder.layer.1.attention.attention.value.bias', 'vit.encoder.layer.1.attention.output.dense.weight', 'vit.encoder.layer.1.attention.output.dense.bias', 'vit.encoder.layer.1.intermediate.dense.weight', 'vit.encoder.layer.1.intermediate.dense.bias', 'vit.encoder.layer.1.output.dense.weight', 'vit.encoder.layer.1.output.dense.bias', 'vit.encoder.layer.1.layernorm_before.weight', 'vit.encoder.layer.1.layernorm_before.bias', 'vit.encoder.layer.1.layernorm_after.weight', 'vit.encoder.layer.1.layernorm_after.bias', 'vit.encoder.layer.2.attention.attention.query.weight', 'vit.encoder.layer.2.attention.attention.query.bias', 'vit.encoder.layer.2.attention.attention.key.weight', 'vit.encoder.layer.2.attention.attention.key.bias', 'vit.encoder.layer.2.attention.attention.value.weight', 'vit.encoder.layer.2.attention.attention.value.bias', 'vit.encoder.layer.2.attention.output.dense.weight', 'vit.encoder.layer.2.attention.output.dense.bias', 'vit.encoder.layer.2.intermediate.dense.weight', 'vit.encoder.layer.2.intermediate.dense.bias', 'vit.encoder.layer.2.output.dense.weight', 'vit.encoder.layer.2.output.dense.bias', 'vit.encoder.layer.2.layernorm_before.weight', 'vit.encoder.layer.2.layernorm_before.bias', 'vit.encoder.layer.2.layernorm_after.weight', 'vit.encoder.layer.2.layernorm_after.bias', 'vit.encoder.layer.3.attention.attention.query.weight', 'vit.encoder.layer.3.attention.attention.query.bias', 'vit.encoder.layer.3.attention.attention.key.weight', 'vit.encoder.layer.3.attention.attention.key.bias', 'vit.encoder.layer.3.attention.attention.value.weight', 'vit.encoder.layer.3.attention.attention.value.bias', 'vit.encoder.layer.3.attention.output.dense.weight', 'vit.encoder.layer.3.attention.output.dense.bias', 'vit.encoder.layer.3.intermediate.dense.weight', 'vit.encoder.layer.3.intermediate.dense.bias', 'vit.encoder.layer.3.output.dense.weight', 'vit.encoder.layer.3.output.dense.bias', 'vit.encoder.layer.3.layernorm_before.weight', 'vit.encoder.layer.3.layernorm_before.bias', 'vit.encoder.layer.3.layernorm_after.weight', 'vit.encoder.layer.3.layernorm_after.bias', 'vit.encoder.layer.4.attention.attention.query.weight', 'vit.encoder.layer.4.attention.attention.query.bias', 'vit.encoder.layer.4.attention.attention.key.weight', 'vit.encoder.layer.4.attention.attention.key.bias', 'vit.encoder.layer.4.attention.attention.value.weight', 'vit.encoder.layer.4.attention.attention.value.bias', 'vit.encoder.layer.4.attention.output.dense.weight', 'vit.encoder.layer.4.attention.output.dense.bias', 'vit.encoder.layer.4.intermediate.dense.weight', 'vit.encoder.layer.4.intermediate.dense.bias', 'vit.encoder.layer.4.output.dense.weight', 'vit.encoder.layer.4.output.dense.bias', 'vit.encoder.layer.4.layernorm_before.weight', 'vit.encoder.layer.4.layernorm_before.bias', 'vit.encoder.layer.4.layernorm_after.weight', 'vit.encoder.layer.4.layernorm_after.bias', 'vit.encoder.layer.5.attention.attention.query.weight', 'vit.encoder.layer.5.attention.attention.query.bias', 'vit.encoder.layer.5.attention.attention.key.weight', 'vit.encoder.layer.5.attention.attention.key.bias', 'vit.encoder.layer.5.attention.attention.value.weight', 'vit.encoder.layer.5.attention.attention.value.bias', 'vit.encoder.layer.5.attention.output.dense.weight', 'vit.encoder.layer.5.attention.output.dense.bias', 'vit.encoder.layer.5.intermediate.dense.weight', 'vit.encoder.layer.5.intermediate.dense.bias', 'vit.encoder.layer.5.output.dense.weight', 'vit.encoder.layer.5.output.dense.bias', 'vit.encoder.layer.5.layernorm_before.weight', 'vit.encoder.layer.5.layernorm_before.bias', 'vit.encoder.layer.5.layernorm_after.weight', 'vit.encoder.layer.5.layernorm_after.bias', 'vit.encoder.layer.6.attention.attention.query.weight', 'vit.encoder.layer.6.attention.attention.query.bias', 'vit.encoder.layer.6.attention.attention.key.weight', 'vit.encoder.layer.6.attention.attention.key.bias', 'vit.encoder.layer.6.attention.attention.value.weight', 'vit.encoder.layer.6.attention.attention.value.bias', 'vit.encoder.layer.6.attention.output.dense.weight', 'vit.encoder.layer.6.attention.output.dense.bias', 'vit.encoder.layer.6.intermediate.dense.weight', 'vit.encoder.layer.6.intermediate.dense.bias', 'vit.encoder.layer.6.output.dense.weight', 'vit.encoder.layer.6.output.dense.bias', 'vit.encoder.layer.6.layernorm_before.weight', 'vit.encoder.layer.6.layernorm_before.bias', 'vit.encoder.layer.6.layernorm_after.weight', 'vit.encoder.layer.6.layernorm_after.bias', 'vit.encoder.layer.7.attention.attention.query.weight', 'vit.encoder.layer.7.attention.attention.query.bias', 'vit.encoder.layer.7.attention.attention.key.weight', 'vit.encoder.layer.7.attention.attention.key.bias', 'vit.encoder.layer.7.attention.attention.value.weight', 'vit.encoder.layer.7.attention.attention.value.bias', 'vit.encoder.layer.7.attention.output.dense.weight', 'vit.encoder.layer.7.attention.output.dense.bias', 'vit.encoder.layer.7.intermediate.dense.weight', 'vit.encoder.layer.7.intermediate.dense.bias', 'vit.encoder.layer.7.output.dense.weight', 'vit.encoder.layer.7.output.dense.bias', 'vit.encoder.layer.7.layernorm_before.weight', 'vit.encoder.layer.7.layernorm_before.bias', 'vit.encoder.layer.7.layernorm_after.weight', 'vit.encoder.layer.7.layernorm_after.bias', 'vit.encoder.layer.8.attention.attention.query.weight', 'vit.encoder.layer.8.attention.attention.query.bias', 'vit.encoder.layer.8.attention.attention.key.weight', 'vit.encoder.layer.8.attention.attention.key.bias', 'vit.encoder.layer.8.attention.attention.value.weight', 'vit.encoder.layer.8.attention.attention.value.bias', 'vit.encoder.layer.8.attention.output.dense.weight', 'vit.encoder.layer.8.attention.output.dense.bias', 'vit.encoder.layer.8.intermediate.dense.weight', 'vit.encoder.layer.8.intermediate.dense.bias', 'vit.encoder.layer.8.output.dense.weight', 'vit.encoder.layer.8.output.dense.bias', 'vit.encoder.layer.8.layernorm_before.weight', 'vit.encoder.layer.8.layernorm_before.bias', 'vit.encoder.layer.8.layernorm_after.weight', 'vit.encoder.layer.8.layernorm_after.bias', 'vit.encoder.layer.9.attention.attention.query.weight', 'vit.encoder.layer.9.attention.attention.query.bias', 'vit.encoder.layer.9.attention.attention.key.weight', 'vit.encoder.layer.9.attention.attention.key.bias', 'vit.encoder.layer.9.attention.attention.value.weight', 'vit.encoder.layer.9.attention.attention.value.bias', 'vit.encoder.layer.9.attention.output.dense.weight', 'vit.encoder.layer.9.attention.output.dense.bias', 'vit.encoder.layer.9.intermediate.dense.weight', 'vit.encoder.layer.9.intermediate.dense.bias', 'vit.encoder.layer.9.output.dense.weight', 'vit.encoder.layer.9.output.dense.bias', 'vit.encoder.layer.9.layernorm_before.weight', 'vit.encoder.layer.9.layernorm_before.bias', 'vit.encoder.layer.9.layernorm_after.weight', 'vit.encoder.layer.9.layernorm_after.bias', 'vit.encoder.layer.10.attention.attention.query.weight', 'vit.encoder.layer.10.attention.attention.query.bias', 'vit.encoder.layer.10.attention.attention.key.weight', 'vit.encoder.layer.10.attention.attention.key.bias', 'vit.encoder.layer.10.attention.attention.value.weight', 'vit.encoder.layer.10.attention.attention.value.bias', 'vit.encoder.layer.10.attention.output.dense.weight', 'vit.encoder.layer.10.attention.output.dense.bias', 'vit.encoder.layer.10.intermediate.dense.weight', 'vit.encoder.layer.10.intermediate.dense.bias', 'vit.encoder.layer.10.output.dense.weight', 'vit.encoder.layer.10.output.dense.bias', 'vit.encoder.layer.10.layernorm_before.weight', 'vit.encoder.layer.10.layernorm_before.bias', 'vit.encoder.layer.10.layernorm_after.weight', 'vit.encoder.layer.10.layernorm_after.bias', 'vit.encoder.layer.11.attention.attention.query.weight', 'vit.encoder.layer.11.attention.attention.query.bias', 'vit.encoder.layer.11.attention.attention.key.weight', 'vit.encoder.layer.11.attention.attention.key.bias', 'vit.encoder.layer.11.attention.attention.value.weight', 'vit.encoder.layer.11.attention.attention.value.bias', 'vit.encoder.layer.11.attention.output.dense.weight', 'vit.encoder.layer.11.attention.output.dense.bias', 'vit.encoder.layer.11.intermediate.dense.weight', 'vit.encoder.layer.11.intermediate.dense.bias', 'vit.encoder.layer.11.output.dense.weight', 'vit.encoder.layer.11.output.dense.bias', 'vit.encoder.layer.11.layernorm_before.weight', 'vit.encoder.layer.11.layernorm_before.bias', 'vit.encoder.layer.11.layernorm_after.weight', 'vit.encoder.layer.11.layernorm_after.bias', 'vit.layernorm.weight', 'vit.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the backbone\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "print(\"Trainable Parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5035790a-3fca-4bc3-8267-ea463db61575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-full\",\n",
    "    per_device_train_batch_size=32,\n",
    "    eval_strategy=\"steps\",  # Updated\n",
    "    num_train_epochs=10,\n",
    "    bf16=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7431e929-fc6a-42a8-8aea-da41974c2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_train_dataset,  # Training dataset\n",
    "    eval_dataset=processed_eval_dataset,  # Validation dataset\n",
    "    processing_class=feature_processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b696dab-764c-4451-b081-9666e3ead49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='880' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [880/880 3:25:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>0.236590</td>\n",
       "      <td>0.574074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.240904</td>\n",
       "      <td>0.575309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.223540</td>\n",
       "      <td>0.608642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.234156</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.239234</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.242853</td>\n",
       "      <td>0.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.244707</td>\n",
       "      <td>0.624691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.249293</td>\n",
       "      <td>0.628395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1c2fc5e-928a-4884-9425-6f4b1e044da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         10.0\n",
      "  total_flos               = 2032381165GF\n",
      "  train_loss               =       0.0734\n",
      "  train_runtime            =   3:25:34.90\n",
      "  train_samples_per_second =        2.283\n",
      "  train_steps_per_second   =        0.071\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de1af8-ff24-4f04-b537-f273898e0e43",
   "metadata": {},
   "source": [
    "# Predict on Test Dataset(Final-Layer-Only Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c0f2272-f42e-4ba9-a200-413c82821cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = ViTForImageClassification.from_pretrained(\"./vit-final-layer-only\")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46ae630c-1212-4111-9142-5533754d9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37542e44-202c-4f3f-be49-f17ae1d4e0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.3984\n",
      "Multilabel Confusion Matrix:\n",
      "[[[174  43]\n",
      "  [ 42 115]]\n",
      "\n",
      " [[ 88 103]\n",
      "  [ 14 169]]\n",
      "\n",
      " [[326   0]\n",
      "  [ 48   0]]\n",
      "\n",
      " [[303   0]\n",
      "  [ 17  54]]\n",
      "\n",
      " [[309   0]\n",
      "  [ 64   1]]\n",
      "\n",
      " [[343   0]\n",
      "  [ 31   0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73       157\n",
      "           1       0.62      0.92      0.74       183\n",
      "           2       0.00      0.00      0.00        48\n",
      "           3       1.00      0.76      0.86        71\n",
      "           4       1.00      0.02      0.03        65\n",
      "           5       0.00      0.00      0.00        31\n",
      "\n",
      "   micro avg       0.70      0.61      0.65       555\n",
      "   macro avg       0.56      0.41      0.39       555\n",
      "weighted avg       0.66      0.61      0.57       555\n",
      " samples avg       0.69      0.65      0.65       555\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsienpanghsieh/ADA/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/hsienpanghsieh/ADA/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "# Perform evaluation\n",
    "results = trainer.predict(test_dataset=processed_test_dataset)\n",
    "\n",
    "# Extract predictions and labels\n",
    "predicted_probs = torch.sigmoid(torch.tensor(results.predictions)).numpy()\n",
    "all_labels = results.label_ids\n",
    "\n",
    "# Apply a threshold to convert probabilities to binary predictions\n",
    "threshold = 0.5\n",
    "all_predictions = (predicted_probs > threshold).astype(int)\n",
    "\n",
    "# Compute exact match accuracy\n",
    "exact_match_accuracy = (all_predictions == all_labels).all(axis=1).mean()\n",
    "print(f\"Exact Match Accuracy: {exact_match_accuracy:.4f}\")\n",
    "\n",
    "# Compute multilabel confusion matrix\n",
    "cm = multilabel_confusion_matrix(all_labels, all_predictions)\n",
    "print(\"Multilabel Confusion Matrix:\")\n",
    "print(cm)\n",
    "print( classification_report(all_labels,all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e980560-fbec-4e89-90a1-c81a8f324e40",
   "metadata": {},
   "source": [
    "# Predict on Test Dataset(Full Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "631a5319-2cae-40da-b128-2488eb6d3875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = ViTForImageClassification.from_pretrained(\"./vit-full\")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b92f8e1-62a0-4885-ad06-a861101039b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4267b1c7-da78-4ced-bce7-6753e80c1c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.5963\n",
      "Multilabel Confusion Matrix:\n",
      "[[[171  46]\n",
      "  [ 12 145]]\n",
      "\n",
      " [[154  37]\n",
      "  [ 19 164]]\n",
      "\n",
      " [[310  16]\n",
      "  [ 20  28]]\n",
      "\n",
      " [[301   2]\n",
      "  [  6  65]]\n",
      "\n",
      " [[299  10]\n",
      "  [ 23  42]]\n",
      "\n",
      " [[341   2]\n",
      "  [  5  26]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.92      0.83       157\n",
      "           1       0.82      0.90      0.85       183\n",
      "           2       0.64      0.58      0.61        48\n",
      "           3       0.97      0.92      0.94        71\n",
      "           4       0.81      0.65      0.72        65\n",
      "           5       0.93      0.84      0.88        31\n",
      "\n",
      "   micro avg       0.81      0.85      0.83       555\n",
      "   macro avg       0.82      0.80      0.81       555\n",
      "weighted avg       0.81      0.85      0.82       555\n",
      " samples avg       0.86      0.89      0.85       555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "# Perform evaluation\n",
    "results = trainer.predict(test_dataset=processed_test_dataset)\n",
    "\n",
    "# Extract predictions and labels\n",
    "predicted_probs = torch.sigmoid(torch.tensor(results.predictions)).numpy()\n",
    "all_labels = results.label_ids\n",
    "\n",
    "# Apply a threshold to convert probabilities to binary predictions\n",
    "threshold = 0.5\n",
    "all_predictions = (predicted_probs > threshold).astype(int)\n",
    "\n",
    "# Compute exact match accuracy\n",
    "exact_match_accuracy = (all_predictions == all_labels).all(axis=1).mean()\n",
    "print(f\"Exact Match Accuracy: {exact_match_accuracy:.4f}\")\n",
    "\n",
    "# Compute multilabel confusion matrix\n",
    "cm = multilabel_confusion_matrix(all_labels, all_predictions)\n",
    "print(\"Multilabel Confusion Matrix:\")\n",
    "print(cm)\n",
    "print( classification_report(all_labels,all_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Deep Learning)",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
